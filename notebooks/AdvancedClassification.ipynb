{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "%matplotlib inline\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "import datetime as dt\n",
    "from datetime import timedelta\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "lc = LabelEncoder()\n",
    "\n",
    "import statsmodels\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RepeatedKFold, GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "from sklearn.metrics import mean_squared_error,r2_score,mean_absolute_error,mean_absolute_percentage_error\n",
    "\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "\n",
    "from numpy import arange\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import plot_confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import roc_curve, auc, plot_roc_curve, roc_auc_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "#import precision and recall\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "# Trick to widen the screen\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "#Widens the code landscape \n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns',500)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_csv(r\"C:\\Users\\Danny Farone\\Documents\\GitHub\\Data-Science-Job-Salaries\\data\\processed\\All Fields\\X_Test.csv\")\n",
    "X_train = pd.read_csv(r\"C:\\Users\\Danny Farone\\Documents\\GitHub\\Data-Science-Job-Salaries\\data\\processed\\All Fields\\X_Train.csv\")\n",
    "y_test = pd.read_csv(r\"C:\\Users\\Danny Farone\\Documents\\GitHub\\Data-Science-Job-Salaries\\data\\processed\\All Fields\\y_Test.csv\")\n",
    "y_train = pd.read_csv(r\"C:\\Users\\Danny Farone\\Documents\\GitHub\\Data-Science-Job-Salaries\\data\\processed\\All Fields\\y_Train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.319672131147541\n",
      "Sensitivity:  0.319672131147541\n",
      "Specificity:  0.42857142857142855\n"
     ]
    }
   ],
   "source": [
    "# Create a base classifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "\n",
    "clf = MultiOutputClassifier(clf)\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "#accuracy of the model on the testing data\n",
    "clf_score_test = clf.score(X_test, y_test)\n",
    "#predictions for the model on the testing data\n",
    "clf_pred_test = clf.predict(X_test)\n",
    "#Precision of the model on the testing data\n",
    "precision_test = precision_score(y_test, clf_pred_test, average = 'micro')\n",
    "#Recall of the model on the testing data\n",
    "recall_test = recall_score(y_test, clf_pred_test, average = 'micro')\n",
    "\n",
    "#Print a simple confusion matrix of the teting data results\n",
    "#print(\"Initial Decision Tree confusion matrix:\")\n",
    "#print(confusion_matrix(y_test, clf_pred_test), '\\n')\n",
    "#Print the scores we calculated earlier in this block\n",
    "print(\"Accuracy: \", clf_score_test)\n",
    "print(\"Sensitivity: \", recall_test)\n",
    "print(\"Specificity: \", precision_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot ROC curve for the first draft model\n",
    "y_pred_prob = clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "\n",
    "plt.plot([0, 1], [0, 1],'k--')\n",
    "plt.plot(fpr, tpr, label='Gradient Boosted Decision Tree')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Decision Tree ROC Curve')\n",
    "plt.show();\n",
    "\n",
    "# calculate roc curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, clf.predict(X_test))\n",
    "\n",
    "# roc auc score\n",
    "log_roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "roc_auc_format = 'ROC AUC Score: {0:.4f}'.format(log_roc_auc)\n",
    "print(roc_auc_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/10 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Define the hyperparameter space to search\n",
    "param_grid = {'estimator__n_estimators': [50, 100, 200, 300, 400, 500],\n",
    "              'estimator__max_features': ['sqrt', 'log2', None],\n",
    "              'estimator__max_depth': [5, 10, 15, 20, None],\n",
    "              'estimator__min_samples_split': [2, 5, 10],\n",
    "              'estimator__min_samples_leaf': [1, 2, 4]}\n",
    "\n",
    "# Create a grid search object\n",
    "grid_search = GridSearchCV(estimator=clf,\n",
    "                           param_grid=param_grid,\n",
    "                           cv=10,\n",
    "                           n_jobs=-1)\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Fit the randomized search object to the training data\n",
    "with tqdm(total=10) as pbar:\n",
    "    for i in range(10):\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        pbar.update(1)\n",
    "\n",
    "# Stop the timer\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "# Print the best hyperparameters and the corresponding best score\n",
    "print(\"Best hyperparameters: \", rand_search.best_params_)\n",
    "print(\"Best score: \", rand_search.best_score_)\n",
    "print(\"Elapsed time: {:.2f} seconds\".format(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit new model with best parameters determined by gscv on the training data\n",
    "clf_gs = RandomForestClassifier(n_estimators=71,\n",
    "                                 max_features='sqrt',\n",
    "                                 max_depth=20,\n",
    "                                 min_samples_split=5,\n",
    "                                 min_samples_leaf=1,\n",
    "                                 random_state=42)\n",
    "clf_gs = MultiOutputClassifier(clf_gs)\n",
    "clf_gs.fit(X_train, y_train)\n",
    "\n",
    "#accuracy of the model on the testing data\n",
    "clf_score_test = clf_gs.score(X_test, y_test)\n",
    "#predictions for the model on the testing data\n",
    "clf_pred_test = clf_gs.predict(X_test)\n",
    "#Precision of the model on the testing data\n",
    "precision_test = precision_score(y_test, clf_pred_test, average = 'micro')\n",
    "#Recall of the model on the testing data\n",
    "recall_test = recall_score(y_test, clf_pred_test, average = 'micro')\n",
    "\n",
    "#Print the scores we calculated earlier in this block\n",
    "print(\"Accuracy: \", clf_score_test)\n",
    "print(\"Sensitivity: \", recall_test)\n",
    "print(\"Specificity: \", precision_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_imp = pd.DataFrame(clf_gs.estimators_[0].feature_importances_)\n",
    "for i in range(1, 4):\n",
    "    feat_imp[i] = pd.DataFrame(clf_gs.estimators_[i].feature_importances_)\n",
    "\n",
    "feat_imp = feat_imp.mean(axis=1)\n",
    "feat_names = pd.DataFrame(list(X.columns))\n",
    "df_feat_imp = pd.concat([feat_imp, feat_names], axis=1)\n",
    "df_feat_imp.columns = ['Importance', 'Features']\n",
    "df_feat_imp.sort_values('Importance', ascending=False, inplace=True)\n",
    "\n",
    "Importance = pd.DataFrame({'Importance': feat_imp * 100}, index=X.columns)\n",
    "Importance.sort_values('Importance', axis=0, ascending=True).plot(kind='barh', color='b')\n",
    "plt.xlabel('Variable Importance')\n",
    "plt.gca().legend_ = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# fit new model with best parameters determined by gscv on the training data\n",
    "clf_gs = RandomForestClassifier(n_estimators=71,\n",
    "                                 max_features='sqrt',\n",
    "                                 max_depth=20,\n",
    "                                 min_samples_split=5,\n",
    "                                 min_samples_leaf=1,\n",
    "                                 random_state=42)\n",
    "clf_gs = MultiOutputClassifier(clf_gs)\n",
    "clf_gs.fit(X_train, y_train)\n",
    "\n",
    "# calculate ROC curve and ROC AUC score for each column of y\n",
    "for i in range(y_test.shape[1]):\n",
    "    # calculate ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_test[:, i], clf_gs.predict_proba(X_test)[:, i, 1])\n",
    "\n",
    "    # plot ROC curve\n",
    "    plt.plot([0, 1], [0, 1],'k--')\n",
    "    plt.plot(fpr, tpr, label='ROC curve for label {}'.format(i))\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve for Label {}'.format(i))\n",
    "    plt.show()\n",
    "\n",
    "    # calculate ROC AUC score\n",
    "    log_roc_auc = roc_auc_score(y_test[:, i], clf_gs.predict_proba(X_test)[:, i, 1])\n",
    "    roc_auc_format = 'ROC AUC Score for label {}: {:.4f}'.format(i, log_roc_auc)\n",
    "    print(roc_auc_format)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predicted labels on the test set\n",
    "y_pred = clf_gs.predict(X_test)\n",
    "\n",
    "# Create a confusion matrix for each target class\n",
    "for i in range(y_test.shape[1]):\n",
    "    cm = confusion_matrix(y_test[:,i], y_pred[:,i])\n",
    "    print(f\"Confusion matrix for target {i}:\")\n",
    "    print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use seaborn to create a heatmap of the confusion matrix from above\n",
    "df_cm = pd.DataFrame(cm)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=1, sharex=True, figsize=(5, 4))\n",
    "fig.suptitle('Confusion Matrix for Classification of Multiple Claims using Logistic Regression', \n",
    "             fontsize=16, y=1.05)\n",
    "ax = plt.subplot()\n",
    "sns.heatmap(df_cm, annot=True, cmap=\"Greens\", annot_kws={\"size\": 16}, ax=ax, fmt=\"g\")\n",
    "ax.set_xlabel('Predicted', fontsize=15)\n",
    "ax.set_ylabel('True', fontsize=15)\n",
    "ax.xaxis.set_ticklabels(['One_Claim', 'Mutliple_Claims'], fontsize=12) \n",
    "ax.yaxis.set_ticklabels(['One_Claim', 'Mutliple_Claims'], fontsize=12, va='center')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
