{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "%matplotlib inline\n",
    "\n",
    "import datetime as dt\n",
    "from datetime import timedelta\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "lc = LabelEncoder()\n",
    "\n",
    "import statsmodels\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RepeatedKFold, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error,r2_score,mean_absolute_error,mean_absolute_percentage_error\n",
    "\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "\n",
    "from numpy import arange\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, BaggingRegressor, RandomForestRegressor\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import plot_confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import roc_curve, auc, plot_roc_curve, roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "\n",
    "#import precision and recall\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "# Trick to widen the screen\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "#Widens the code landscape \n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns',500)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_csv(r\"C:\\Users\\Danny Farone\\Documents\\GitHub\\Data-Science-Job-Salaries\\data\\processed\\DataAnalyst_X_Test.csv\")\n",
    "X_train = pd.read_csv(r\"C:\\Users\\Danny Farone\\Documents\\GitHub\\Data-Science-Job-Salaries\\data\\processed\\DataAnalyst_X_Train.csv\")\n",
    "y_test = pd.read_csv(r\"C:\\Users\\Danny Farone\\Documents\\GitHub\\Data-Science-Job-Salaries\\data\\processed\\DataAnalyst_y_Test.csv\")\n",
    "y_train = pd.read_csv(r\"C:\\Users\\Danny Farone\\Documents\\GitHub\\Data-Science-Job-Salaries\\data\\processed\\DataAnalyst_y_Train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit initial model on the training data\n",
    "logistic = LogisticRegression(max_iter = 10000, random_state = 42)\n",
    "logistic.fit(X_train, y_train)\n",
    "\n",
    "#accuracy of the model on the testing data\n",
    "log_score_test = logistic.score(X_test, y_test)\n",
    "#predictions for the model on the testing data\n",
    "log_pred_test = logistic.predict(X_test)\n",
    "#Precision of the model on the testing data\n",
    "log_precision_test = precision_score(y_test, log_pred_test)\n",
    "#Recall of the model on the testing data\n",
    "log_recall_test = recall_score(y_test, log_pred_test)\n",
    "\n",
    "#Print a simple confusion matrix of the teting data results\n",
    "print(\"Logistic Regression Model Testing Data Confusion Matrix :\")\n",
    "print(confusion_matrix(y_test, log_pred_test)) \n",
    "#Print the scores\n",
    "print(\"Accuracy: \", log_score_test)\n",
    "print(\"Sensitivity: \", log_recall_test)\n",
    "print(\"Specificity: \", log_precision_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set a variable for the logistic intercept\n",
    "log_intercept = logistic.intercept_ \n",
    "beta_0 = log_intercept\n",
    "\n",
    "# extract log reg coefs\n",
    "coef = logistic.coef_[0]\n",
    "coef = np.array(coef)\n",
    "df_coef = pd.DataFrame(coef)\n",
    "df_coef = df_coef.T # transpose to match column names\n",
    "\n",
    "# column names\n",
    "names = X.columns\n",
    "df_coef.columns = names\n",
    "df_coef = df_coef.T # transpose for better view\n",
    "\n",
    "# sort coefficients in ascending order\n",
    "df_coef = df_coef.rename(columns = {0:'logregCV_coeff'})\n",
    "df_coef = df_coef.sort_values('logregCV_coeff')\n",
    "df_coef = df_coef.reset_index()\n",
    "df_coef = df_coef.rename(columns = {'index':'Variable_Names', 'logreg_coeff':'logregCV_coeff'})\n",
    "df_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display barplot of log reg coefs\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x=\"Variable_Names\",y=\"logregCV_coeff\",data= df_coef)\n",
    "plt.xticks(rotation=80)\n",
    "plt.title('The Barplot of Coefficients of Logistic Regression Model')\n",
    "plt.xlabel('Variable Names')\n",
    "plt.ylabel('Value of Coefficient')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataframe of the the True values, False values, True Predictions, and False Predicitons for a confusion matrix\n",
    "cm = pd.DataFrame(confusion_matrix(y_test, log_pred_test), index=['One_Claim','Mutliple_Claims'], \n",
    "                  columns=['One_Claim','Mutliple_Claims'])\n",
    "cm.index.name = 'True'\n",
    "cm.columns.name = 'Predicted'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use seaborn to create a heatmap of the confusion matrix from above\n",
    "df_cm = pd.DataFrame(cm)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=1, sharex=True, figsize=(5, 4))\n",
    "fig.suptitle('Confusion Matrix for Classification of Multiple Claims using Logistic Regression', \n",
    "             fontsize=16, y=1.05)\n",
    "ax = plt.subplot()\n",
    "sns.heatmap(df_cm, annot=True, cmap=\"Greens\", annot_kws={\"size\": 16}, ax=ax, fmt=\"g\")\n",
    "ax.set_xlabel('Predicted', fontsize=15)\n",
    "ax.set_ylabel('True', fontsize=15)\n",
    "ax.xaxis.set_ticklabels(['One_Claim', 'Mutliple_Claims'], fontsize=12) \n",
    "ax.yaxis.set_ticklabels(['One_Claim', 'Mutliple_Claims'], fontsize=12, va='center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot ROC curve\n",
    "y_pred_prob = logistic.predict_proba(X_test)[:,1]\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "\n",
    "plt.plot([0, 1], [0, 1],'k--')\n",
    "plt.plot(fpr, tpr, label='Logistic Regression')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Decision Tree ROC Curve')\n",
    "plt.show();\n",
    "\n",
    "# calculate roc curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, logistic.predict(X_test))\n",
    "\n",
    "# roc auc score\n",
    "log_roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "roc_auc_format = 'ROC AUC Score: {0:.4f}'.format(log_roc_auc)\n",
    "print(roc_auc_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores = pd.DataFrame({'Measures':['Accuracy','Sensitivity','Specificity','AUC'],'Logistic':[log_score,log_recall,log_precision,log_roc_auc]})\n",
    "df_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_csv(r\"C:\\Users\\Danny Farone\\Documents\\GitHub\\Data-Science-Job-Salaries\\data\\processed\\DataEngineer_X_Test.csv\")\n",
    "X_train = pd.read_csv(r\"C:\\Users\\Danny Farone\\Documents\\GitHub\\Data-Science-Job-Salaries\\data\\processed\\DataEngineer_X_Train.csv\")\n",
    "y_test = pd.read_csv(r\"C:\\Users\\Danny Farone\\Documents\\GitHub\\Data-Science-Job-Salaries\\data\\processed\\DataEngineer_y_Test.csv\")\n",
    "y_train = pd.read_csv(r\"C:\\Users\\Danny Farone\\Documents\\GitHub\\Data-Science-Job-Salaries\\data\\processed\\DataEngineer_y_Train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit initial model on the training data\n",
    "logistic = LogisticRegression(max_iter = 10000, random_state = 42)\n",
    "logistic.fit(X_train, y_train)\n",
    "\n",
    "#accuracy of the model on the testing data\n",
    "log_score_test = logistic.score(X_test, y_test)\n",
    "#predictions for the model on the testing data\n",
    "log_pred_test = logistic.predict(X_test)\n",
    "#Precision of the model on the testing data\n",
    "log_precision_test = precision_score(y_test, log_pred_test)\n",
    "#Recall of the model on the testing data\n",
    "log_recall_test = recall_score(y_test, log_pred_test)\n",
    "\n",
    "#Print a simple confusion matrix of the teting data results\n",
    "print(\"Logistic Regression Model Testing Data Confusion Matrix :\")\n",
    "print(confusion_matrix(y_test, log_pred_test)) \n",
    "#Print the scores\n",
    "print(\"Accuracy: \", log_score_test)\n",
    "print(\"Sensitivity: \", log_recall_test)\n",
    "print(\"Specificity: \", log_precision_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set a variable for the logistic intercept\n",
    "log_intercept = logistic.intercept_ \n",
    "beta_0 = log_intercept\n",
    "\n",
    "# extract log reg coefs\n",
    "coef = logistic.coef_[0]\n",
    "coef = np.array(coef)\n",
    "df_coef = pd.DataFrame(coef)\n",
    "df_coef = df_coef.T # transpose to match column names\n",
    "\n",
    "# column names\n",
    "names = X.columns\n",
    "df_coef.columns = names\n",
    "df_coef = df_coef.T # transpose for better view\n",
    "\n",
    "# sort coefficients in ascending order\n",
    "df_coef = df_coef.rename(columns = {0:'logregCV_coeff'})\n",
    "df_coef = df_coef.sort_values('logregCV_coeff')\n",
    "df_coef = df_coef.reset_index()\n",
    "df_coef = df_coef.rename(columns = {'index':'Variable_Names', 'logreg_coeff':'logregCV_coeff'})\n",
    "df_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display barplot of log reg coefs\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x=\"Variable_Names\",y=\"logregCV_coeff\",data= df_coef)\n",
    "plt.xticks(rotation=80)\n",
    "plt.title('The Barplot of Coefficients of Logistic Regression Model')\n",
    "plt.xlabel('Variable Names')\n",
    "plt.ylabel('Value of Coefficient')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataframe of the the True values, False values, True Predictions, and False Predicitons for a confusion matrix\n",
    "cm = pd.DataFrame(confusion_matrix(y_test, log_pred_test), index=['One_Claim','Mutliple_Claims'], \n",
    "                  columns=['One_Claim','Mutliple_Claims'])\n",
    "cm.index.name = 'True'\n",
    "cm.columns.name = 'Predicted'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use seaborn to create a heatmap of the confusion matrix from above\n",
    "df_cm = pd.DataFrame(cm)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=1, sharex=True, figsize=(5, 4))\n",
    "fig.suptitle('Confusion Matrix for Classification of Multiple Claims using Logistic Regression', \n",
    "             fontsize=16, y=1.05)\n",
    "ax = plt.subplot()\n",
    "sns.heatmap(df_cm, annot=True, cmap=\"Greens\", annot_kws={\"size\": 16}, ax=ax, fmt=\"g\")\n",
    "ax.set_xlabel('Predicted', fontsize=15)\n",
    "ax.set_ylabel('True', fontsize=15)\n",
    "ax.xaxis.set_ticklabels(['One_Claim', 'Mutliple_Claims'], fontsize=12) \n",
    "ax.yaxis.set_ticklabels(['One_Claim', 'Mutliple_Claims'], fontsize=12, va='center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot ROC curve\n",
    "y_pred_prob = logistic.predict_proba(X_test)[:,1]\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "\n",
    "plt.plot([0, 1], [0, 1],'k--')\n",
    "plt.plot(fpr, tpr, label='Logistic Regression')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Decision Tree ROC Curve')\n",
    "plt.show();\n",
    "\n",
    "# calculate roc curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, logistic.predict(X_test))\n",
    "\n",
    "# roc auc score\n",
    "log_roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "roc_auc_format = 'ROC AUC Score: {0:.4f}'.format(log_roc_auc)\n",
    "print(roc_auc_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores = pd.DataFrame({'Measures':['Accuracy','Sensitivity','Specificity','AUC'],'Logistic':[log_score,log_recall,log_precision,log_roc_auc]})\n",
    "df_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_csv(r\"C:\\Users\\Danny Farone\\Documents\\GitHub\\Data-Science-Job-Salaries\\data\\processed\\DataScientist_X_Test.csv\")\n",
    "X_train = pd.read_csv(r\"C:\\Users\\Danny Farone\\Documents\\GitHub\\Data-Science-Job-Salaries\\data\\processed\\DataScientist_X_Train.csv\")\n",
    "y_test = pd.read_csv(r\"C:\\Users\\Danny Farone\\Documents\\GitHub\\Data-Science-Job-Salaries\\data\\processed\\DataScientist_y_Test.csv\")\n",
    "y_train = pd.read_csv(r\"C:\\Users\\Danny Farone\\Documents\\GitHub\\Data-Science-Job-Salaries\\data\\processed\\DataScientist_y_Train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit initial model on the training data\n",
    "logistic = LogisticRegression(max_iter = 10000, random_state = 42)\n",
    "logistic.fit(X_train, y_train)\n",
    "\n",
    "#accuracy of the model on the testing data\n",
    "log_score_test = logistic.score(X_test, y_test)\n",
    "#predictions for the model on the testing data\n",
    "log_pred_test = logistic.predict(X_test)\n",
    "#Precision of the model on the testing data\n",
    "log_precision_test = precision_score(y_test, log_pred_test)\n",
    "#Recall of the model on the testing data\n",
    "log_recall_test = recall_score(y_test, log_pred_test)\n",
    "\n",
    "#Print a simple confusion matrix of the teting data results\n",
    "print(\"Logistic Regression Model Testing Data Confusion Matrix :\")\n",
    "print(confusion_matrix(y_test, log_pred_test)) \n",
    "#Print the scores\n",
    "print(\"Accuracy: \", log_score_test)\n",
    "print(\"Sensitivity: \", log_recall_test)\n",
    "print(\"Specificity: \", log_precision_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set a variable for the logistic intercept\n",
    "log_intercept = logistic.intercept_ \n",
    "beta_0 = log_intercept\n",
    "\n",
    "# extract log reg coefs\n",
    "coef = logistic.coef_[0]\n",
    "coef = np.array(coef)\n",
    "df_coef = pd.DataFrame(coef)\n",
    "df_coef = df_coef.T # transpose to match column names\n",
    "\n",
    "# column names\n",
    "names = X.columns\n",
    "df_coef.columns = names\n",
    "df_coef = df_coef.T # transpose for better view\n",
    "\n",
    "# sort coefficients in ascending order\n",
    "df_coef = df_coef.rename(columns = {0:'logregCV_coeff'})\n",
    "df_coef = df_coef.sort_values('logregCV_coeff')\n",
    "df_coef = df_coef.reset_index()\n",
    "df_coef = df_coef.rename(columns = {'index':'Variable_Names', 'logreg_coeff':'logregCV_coeff'})\n",
    "df_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display barplot of log reg coefs\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x=\"Variable_Names\",y=\"logregCV_coeff\",data= df_coef)\n",
    "plt.xticks(rotation=80)\n",
    "plt.title('The Barplot of Coefficients of Logistic Regression Model')\n",
    "plt.xlabel('Variable Names')\n",
    "plt.ylabel('Value of Coefficient')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataframe of the the True values, False values, True Predictions, and False Predicitons for a confusion matrix\n",
    "cm = pd.DataFrame(confusion_matrix(y_test, log_pred_test), index=['One_Claim','Mutliple_Claims'], \n",
    "                  columns=['One_Claim','Mutliple_Claims'])\n",
    "cm.index.name = 'True'\n",
    "cm.columns.name = 'Predicted'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot ROC curve\n",
    "y_pred_prob = logistic.predict_proba(X_test)[:,1]\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "\n",
    "plt.plot([0, 1], [0, 1],'k--')\n",
    "plt.plot(fpr, tpr, label='Logistic Regression')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Decision Tree ROC Curve')\n",
    "plt.show();\n",
    "\n",
    "# calculate roc curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, logistic.predict(X_test))\n",
    "\n",
    "# roc auc score\n",
    "log_roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "roc_auc_format = 'ROC AUC Score: {0:.4f}'.format(log_roc_auc)\n",
    "print(roc_auc_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores = pd.DataFrame({'Measures':['Accuracy','Sensitivity','Specificity','AUC'],'Logistic':[log_score,log_recall,log_precision,log_roc_auc]})\n",
    "df_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_csv(r\"C:\\Users\\Danny Farone\\Documents\\GitHub\\Data-Science-Job-Salaries\\data\\processed\\Machine_Learning_Engineer_X_Test.csv\")\n",
    "X_train = pd.read_csv(r\"C:\\Users\\Danny Farone\\Documents\\GitHub\\Data-Science-Job-Salaries\\data\\processed\\Machine_Learning_Engineer_y_Test.csv\")\n",
    "y_test = pd.read_csv(r\"C:\\Users\\Danny Farone\\Documents\\GitHub\\Data-Science-Job-Salaries\\data\\processed\\Machine_Learning_Engineer_y_Train.csv\")\n",
    "y_train = pd.read_csv(r\"C:\\Users\\Danny Farone\\Documents\\GitHub\\Data-Science-Job-Salaries\\data\\processed\\Machine_Learning_Engineerr_X_Train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit initial model on the training data\n",
    "logistic = LogisticRegression(max_iter = 10000, random_state = 42)\n",
    "logistic.fit(X_train, y_train)\n",
    "\n",
    "#accuracy of the model on the testing data\n",
    "log_score_test = logistic.score(X_test, y_test)\n",
    "#predictions for the model on the testing data\n",
    "log_pred_test = logistic.predict(X_test)\n",
    "#Precision of the model on the testing data\n",
    "log_precision_test = precision_score(y_test, log_pred_test)\n",
    "#Recall of the model on the testing data\n",
    "log_recall_test = recall_score(y_test, log_pred_test)\n",
    "\n",
    "#Print a simple confusion matrix of the teting data results\n",
    "print(\"Logistic Regression Model Testing Data Confusion Matrix :\")\n",
    "print(confusion_matrix(y_test, log_pred_test)) \n",
    "#Print the scores\n",
    "print(\"Accuracy: \", log_score_test)\n",
    "print(\"Sensitivity: \", log_recall_test)\n",
    "print(\"Specificity: \", log_precision_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set a variable for the logistic intercept\n",
    "log_intercept = logistic.intercept_ \n",
    "beta_0 = log_intercept\n",
    "\n",
    "# extract log reg coefs\n",
    "coef = logistic.coef_[0]\n",
    "coef = np.array(coef)\n",
    "df_coef = pd.DataFrame(coef)\n",
    "df_coef = df_coef.T # transpose to match column names\n",
    "\n",
    "# column names\n",
    "names = X.columns\n",
    "df_coef.columns = names\n",
    "df_coef = df_coef.T # transpose for better view\n",
    "\n",
    "# sort coefficients in ascending order\n",
    "df_coef = df_coef.rename(columns = {0:'logregCV_coeff'})\n",
    "df_coef = df_coef.sort_values('logregCV_coeff')\n",
    "df_coef = df_coef.reset_index()\n",
    "df_coef = df_coef.rename(columns = {'index':'Variable_Names', 'logreg_coeff':'logregCV_coeff'})\n",
    "df_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display barplot of log reg coefs\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x=\"Variable_Names\",y=\"logregCV_coeff\",data= df_coef)\n",
    "plt.xticks(rotation=80)\n",
    "plt.title('The Barplot of Coefficients of Logistic Regression Model')\n",
    "plt.xlabel('Variable Names')\n",
    "plt.ylabel('Value of Coefficient')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataframe of the the True values, False values, True Predictions, and False Predicitons for a confusion matrix\n",
    "cm = pd.DataFrame(confusion_matrix(y_test, log_pred_test), index=['One_Claim','Mutliple_Claims'], \n",
    "                  columns=['One_Claim','Mutliple_Claims'])\n",
    "cm.index.name = 'True'\n",
    "cm.columns.name = 'Predicted'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot ROC curve\n",
    "y_pred_prob = logistic.predict_proba(X_test)[:,1]\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "\n",
    "plt.plot([0, 1], [0, 1],'k--')\n",
    "plt.plot(fpr, tpr, label='Logistic Regression')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Decision Tree ROC Curve')\n",
    "plt.show();\n",
    "\n",
    "# calculate roc curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, logistic.predict(X_test))\n",
    "\n",
    "# roc auc score\n",
    "log_roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "roc_auc_format = 'ROC AUC Score: {0:.4f}'.format(log_roc_auc)\n",
    "print(roc_auc_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores = pd.DataFrame({'Measures':['Accuracy','Sensitivity','Specificity','AUC'],'Logistic':[log_score,log_recall,log_precision,log_roc_auc]})\n",
    "df_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit new model with best parameters determined by gscv on the training data\n",
    "decTree_gs = GradientBoostingClassifier(random_state =42, learning_rate =0.01, max_depth =3, max_leaf_nodes =2, n_estimators =50)\n",
    "decTree_gs.fit(X_train, y_train)\n",
    "\n",
    "#accuracy of the model on the testing data\n",
    "decTree_score_test = decTree_gs.score(X_test, y_test)\n",
    "#predictions for the model on the testing data\n",
    "decTree_pred_test = decTree_gs.predict(X_test)\n",
    "#Precision of the model on the testing data\n",
    "decTree_precision_test = precision_score(y_test, decTree_pred_test)\n",
    "#Recall of the model on the testing data\n",
    "decTree_recall_test = recall_score(y_test, decTree_pred_test)\n",
    "\n",
    "#Print a simple confusion matrix of the teting data results\n",
    "print(\"Decision Tree confusion matrix:\")\n",
    "print(confusion_matrix(y_test, decTree_pred_test), '\\n')\n",
    "\n",
    "#Print the scores\n",
    "print(\"Accuracy: \", decTree_score_test)\n",
    "print(\"Sensitivity: \", recall_test)\n",
    "print(\"Specificity: \", precision_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
