{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"C:/Users/Danny Farone/Documents/GitHub/Data-Science-Job-Salaries/src/\")\n",
    "from required_libraries import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_csv(r\"C:\\Users\\Danny Farone\\Documents\\GitHub\\Data-Science-Job-Salaries\\data\\processed\\Data Analyst\\DataAnalyst_X_Test.csv\")\n",
    "X_train = pd.read_csv(r\"C:\\Users\\Danny Farone\\Documents\\GitHub\\Data-Science-Job-Salaries\\data\\processed\\Data Analyst\\DataAnalyst_X_Train.csv\")\n",
    "y_test = pd.read_csv(r\"C:\\Users\\Danny Farone\\Documents\\GitHub\\Data-Science-Job-Salaries\\data\\processed\\Data Analyst\\DataAnalyst_y_Test.csv\")\n",
    "y_train = pd.read_csv(r\"C:\\Users\\Danny Farone\\Documents\\GitHub\\Data-Science-Job-Salaries\\data\\processed\\Data Analyst\\DataAnalyst_y_Train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model Testing Data Confusion Matrix :\n",
      "[[94  0]\n",
      " [28  0]]\n",
      "Accuracy:  0.7704918032786885\n",
      "Sensitivity:  0.0\n",
      "Specificity:  0.0\n"
     ]
    }
   ],
   "source": [
    "# fit initial model on the training data\n",
    "logistic = LogisticRegression(max_iter = 10000, random_state = 42)\n",
    "logistic.fit(X_train, y_train)\n",
    "\n",
    "#accuracy of the model on the testing data\n",
    "log_score_test = logistic.score(X_test, y_test)\n",
    "#predictions for the model on the testing data\n",
    "log_pred_test = logistic.predict(X_test)\n",
    "#Precision of the model on the testing data\n",
    "log_precision_test = precision_score(y_test, log_pred_test)\n",
    "#Recall of the model on the testing data\n",
    "log_recall_test = recall_score(y_test, log_pred_test)\n",
    "\n",
    "#Print a simple confusion matrix of the teting data results\n",
    "print(\"Logistic Regression Model Testing Data Confusion Matrix :\")\n",
    "print(confusion_matrix(y_test, log_pred_test)) \n",
    "#Print the scores\n",
    "print(\"Accuracy: \", log_score_test)\n",
    "print(\"Sensitivity: \", log_recall_test)\n",
    "print(\"Specificity: \", log_precision_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set a variable for the logistic intercept\n",
    "log_intercept = logistic.intercept_ \n",
    "beta_0 = log_intercept\n",
    "\n",
    "# extract log reg coefs\n",
    "coef = logistic.coef_[0]\n",
    "coef = np.array(coef)\n",
    "df_coef = pd.DataFrame(coef)\n",
    "df_coef = df_coef.T # transpose to match column names\n",
    "\n",
    "# column names\n",
    "names = X.columns\n",
    "df_coef.columns = names\n",
    "df_coef = df_coef.T # transpose for better view\n",
    "\n",
    "# sort coefficients in ascending order\n",
    "df_coef = df_coef.rename(columns = {0:'logregCV_coeff'})\n",
    "df_coef = df_coef.sort_values('logregCV_coeff')\n",
    "df_coef = df_coef.reset_index()\n",
    "df_coef = df_coef.rename(columns = {'index':'Variable_Names', 'logreg_coeff':'logregCV_coeff'})\n",
    "df_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display barplot of log reg coefs\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x=\"Variable_Names\",y=\"logregCV_coeff\",data= df_coef)\n",
    "plt.xticks(rotation=80)\n",
    "plt.title('The Barplot of Coefficients of Logistic Regression Model')\n",
    "plt.xlabel('Variable Names')\n",
    "plt.ylabel('Value of Coefficient')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataframe of the the True values, False values, True Predictions, and False Predicitons for a confusion matrix\n",
    "cm = pd.DataFrame(confusion_matrix(y_test, log_pred_test), index=['One_Claim','Mutliple_Claims'], \n",
    "                  columns=['One_Claim','Mutliple_Claims'])\n",
    "cm.index.name = 'True'\n",
    "cm.columns.name = 'Predicted'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use seaborn to create a heatmap of the confusion matrix from above\n",
    "df_cm = pd.DataFrame(cm)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=1, sharex=True, figsize=(5, 4))\n",
    "fig.suptitle('Confusion Matrix for Classification of Multiple Claims using Logistic Regression', \n",
    "             fontsize=16, y=1.05)\n",
    "ax = plt.subplot()\n",
    "sns.heatmap(df_cm, annot=True, cmap=\"Greens\", annot_kws={\"size\": 16}, ax=ax, fmt=\"g\")\n",
    "ax.set_xlabel('Predicted', fontsize=15)\n",
    "ax.set_ylabel('True', fontsize=15)\n",
    "ax.xaxis.set_ticklabels(['One_Claim', 'Mutliple_Claims'], fontsize=12) \n",
    "ax.yaxis.set_ticklabels(['One_Claim', 'Mutliple_Claims'], fontsize=12, va='center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot ROC curve\n",
    "y_pred_prob = logistic.predict_proba(X_test)[:,1]\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "\n",
    "plt.plot([0, 1], [0, 1],'k--')\n",
    "plt.plot(fpr, tpr, label='Logistic Regression')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Decision Tree ROC Curve')\n",
    "plt.show();\n",
    "\n",
    "# calculate roc curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, logistic.predict(X_test))\n",
    "\n",
    "# roc auc score\n",
    "log_roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "roc_auc_format = 'ROC AUC Score: {0:.4f}'.format(log_roc_auc)\n",
    "print(roc_auc_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores = pd.DataFrame({'Measures':['Accuracy','Sensitivity','Specificity','AUC'],'Logistic':[log_score,log_recall,log_precision,log_roc_auc]})\n",
    "df_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_csv(r\"C:\\Users\\Danny Farone\\Documents\\GitHub\\Data-Science-Job-Salaries\\data\\processed\\Data Engineer\\DataEngineer_X_Test.csv\")\n",
    "X_train = pd.read_csv(r\"C:\\Users\\Danny Farone\\Documents\\GitHub\\Data-Science-Job-Salaries\\data\\processed\\Data Engineer\\DataEngineer_X_Train.csv\")\n",
    "y_test = pd.read_csv(r\"C:\\Users\\Danny Farone\\Documents\\GitHub\\Data-Science-Job-Salaries\\data\\processed\\Data Engineer\\DataEngineer_y_Test.csv\")\n",
    "y_train = pd.read_csv(r\"C:\\Users\\Danny Farone\\Documents\\GitHub\\Data-Science-Job-Salaries\\data\\processed\\Data Engineer\\DataEngineer_y_Train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit initial model on the training data\n",
    "logistic = LogisticRegression(max_iter = 10000, random_state = 42)\n",
    "logistic.fit(X_train, y_train)\n",
    "\n",
    "#accuracy of the model on the testing data\n",
    "log_score_test = logistic.score(X_test, y_test)\n",
    "#predictions for the model on the testing data\n",
    "log_pred_test = logistic.predict(X_test)\n",
    "#Precision of the model on the testing data\n",
    "log_precision_test = precision_score(y_test, log_pred_test)\n",
    "#Recall of the model on the testing data\n",
    "log_recall_test = recall_score(y_test, log_pred_test)\n",
    "\n",
    "#Print a simple confusion matrix of the teting data results\n",
    "print(\"Logistic Regression Model Testing Data Confusion Matrix :\")\n",
    "print(confusion_matrix(y_test, log_pred_test)) \n",
    "#Print the scores\n",
    "print(\"Accuracy: \", log_score_test)\n",
    "print(\"Sensitivity: \", log_recall_test)\n",
    "print(\"Specificity: \", log_precision_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set a variable for the logistic intercept\n",
    "log_intercept = logistic.intercept_ \n",
    "beta_0 = log_intercept\n",
    "\n",
    "# extract log reg coefs\n",
    "coef = logistic.coef_[0]\n",
    "coef = np.array(coef)\n",
    "df_coef = pd.DataFrame(coef)\n",
    "df_coef = df_coef.T # transpose to match column names\n",
    "\n",
    "# column names\n",
    "names = X.columns\n",
    "df_coef.columns = names\n",
    "df_coef = df_coef.T # transpose for better view\n",
    "\n",
    "# sort coefficients in ascending order\n",
    "df_coef = df_coef.rename(columns = {0:'logregCV_coeff'})\n",
    "df_coef = df_coef.sort_values('logregCV_coeff')\n",
    "df_coef = df_coef.reset_index()\n",
    "df_coef = df_coef.rename(columns = {'index':'Variable_Names', 'logreg_coeff':'logregCV_coeff'})\n",
    "df_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display barplot of log reg coefs\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x=\"Variable_Names\",y=\"logregCV_coeff\",data= df_coef)\n",
    "plt.xticks(rotation=80)\n",
    "plt.title('The Barplot of Coefficients of Logistic Regression Model')\n",
    "plt.xlabel('Variable Names')\n",
    "plt.ylabel('Value of Coefficient')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataframe of the the True values, False values, True Predictions, and False Predicitons for a confusion matrix\n",
    "cm = pd.DataFrame(confusion_matrix(y_test, log_pred_test), index=['One_Claim','Mutliple_Claims'], \n",
    "                  columns=['One_Claim','Mutliple_Claims'])\n",
    "cm.index.name = 'True'\n",
    "cm.columns.name = 'Predicted'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use seaborn to create a heatmap of the confusion matrix from above\n",
    "df_cm = pd.DataFrame(cm)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=1, sharex=True, figsize=(5, 4))\n",
    "fig.suptitle('Confusion Matrix for Classification of Multiple Claims using Logistic Regression', \n",
    "             fontsize=16, y=1.05)\n",
    "ax = plt.subplot()\n",
    "sns.heatmap(df_cm, annot=True, cmap=\"Greens\", annot_kws={\"size\": 16}, ax=ax, fmt=\"g\")\n",
    "ax.set_xlabel('Predicted', fontsize=15)\n",
    "ax.set_ylabel('True', fontsize=15)\n",
    "ax.xaxis.set_ticklabels(['One_Claim', 'Mutliple_Claims'], fontsize=12) \n",
    "ax.yaxis.set_ticklabels(['One_Claim', 'Mutliple_Claims'], fontsize=12, va='center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot ROC curve\n",
    "y_pred_prob = logistic.predict_proba(X_test)[:,1]\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "\n",
    "plt.plot([0, 1], [0, 1],'k--')\n",
    "plt.plot(fpr, tpr, label='Logistic Regression')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Decision Tree ROC Curve')\n",
    "plt.show();\n",
    "\n",
    "# calculate roc curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, logistic.predict(X_test))\n",
    "\n",
    "# roc auc score\n",
    "log_roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "roc_auc_format = 'ROC AUC Score: {0:.4f}'.format(log_roc_auc)\n",
    "print(roc_auc_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores = pd.DataFrame({'Measures':['Accuracy','Sensitivity','Specificity','AUC'],'Logistic':[log_score,log_recall,log_precision,log_roc_auc]})\n",
    "df_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_csv(r\"C:\\Users\\Danny Farone\\Documents\\GitHub\\Data-Science-Job-Salaries\\data\\processed\\Data Scientist\\DataScientist_X_Test.csv\")\n",
    "X_train = pd.read_csv(r\"C:\\Users\\Danny Farone\\Documents\\GitHub\\Data-Science-Job-Salaries\\data\\processed\\Data Scientist\\DataScientist_X_Train.csv\")\n",
    "y_test = pd.read_csv(r\"C:\\Users\\Danny Farone\\Documents\\GitHub\\Data-Science-Job-Salaries\\data\\processed\\Data Scientist\\DataScientist_y_Test.csv\")\n",
    "y_train = pd.read_csv(r\"C:\\Users\\Danny Farone\\Documents\\GitHub\\Data-Science-Job-Salaries\\data\\processed\\Data Scientist\\DataScientist_y_Train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit initial model on the training data\n",
    "logistic = LogisticRegression(max_iter = 10000, random_state = 42)\n",
    "logistic.fit(X_train, y_train)\n",
    "\n",
    "#accuracy of the model on the testing data\n",
    "log_score_test = logistic.score(X_test, y_test)\n",
    "#predictions for the model on the testing data\n",
    "log_pred_test = logistic.predict(X_test)\n",
    "#Precision of the model on the testing data\n",
    "log_precision_test = precision_score(y_test, log_pred_test)\n",
    "#Recall of the model on the testing data\n",
    "log_recall_test = recall_score(y_test, log_pred_test)\n",
    "\n",
    "#Print a simple confusion matrix of the teting data results\n",
    "print(\"Logistic Regression Model Testing Data Confusion Matrix :\")\n",
    "print(confusion_matrix(y_test, log_pred_test)) \n",
    "#Print the scores\n",
    "print(\"Accuracy: \", log_score_test)\n",
    "print(\"Sensitivity: \", log_recall_test)\n",
    "print(\"Specificity: \", log_precision_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set a variable for the logistic intercept\n",
    "log_intercept = logistic.intercept_ \n",
    "beta_0 = log_intercept\n",
    "\n",
    "# extract log reg coefs\n",
    "coef = logistic.coef_[0]\n",
    "coef = np.array(coef)\n",
    "df_coef = pd.DataFrame(coef)\n",
    "df_coef = df_coef.T # transpose to match column names\n",
    "\n",
    "# column names\n",
    "names = X.columns\n",
    "df_coef.columns = names\n",
    "df_coef = df_coef.T # transpose for better view\n",
    "\n",
    "# sort coefficients in ascending order\n",
    "df_coef = df_coef.rename(columns = {0:'logregCV_coeff'})\n",
    "df_coef = df_coef.sort_values('logregCV_coeff')\n",
    "df_coef = df_coef.reset_index()\n",
    "df_coef = df_coef.rename(columns = {'index':'Variable_Names', 'logreg_coeff':'logregCV_coeff'})\n",
    "df_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display barplot of log reg coefs\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x=\"Variable_Names\",y=\"logregCV_coeff\",data= df_coef)\n",
    "plt.xticks(rotation=80)\n",
    "plt.title('The Barplot of Coefficients of Logistic Regression Model')\n",
    "plt.xlabel('Variable Names')\n",
    "plt.ylabel('Value of Coefficient')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataframe of the the True values, False values, True Predictions, and False Predicitons for a confusion matrix\n",
    "cm = pd.DataFrame(confusion_matrix(y_test, log_pred_test), index=['One_Claim','Mutliple_Claims'], \n",
    "                  columns=['One_Claim','Mutliple_Claims'])\n",
    "cm.index.name = 'True'\n",
    "cm.columns.name = 'Predicted'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot ROC curve\n",
    "y_pred_prob = logistic.predict_proba(X_test)[:,1]\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "\n",
    "plt.plot([0, 1], [0, 1],'k--')\n",
    "plt.plot(fpr, tpr, label='Logistic Regression')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Decision Tree ROC Curve')\n",
    "plt.show();\n",
    "\n",
    "# calculate roc curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, logistic.predict(X_test))\n",
    "\n",
    "# roc auc score\n",
    "log_roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "roc_auc_format = 'ROC AUC Score: {0:.4f}'.format(log_roc_auc)\n",
    "print(roc_auc_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores = pd.DataFrame({'Measures':['Accuracy','Sensitivity','Specificity','AUC'],'Logistic':[log_score,log_recall,log_precision,log_roc_auc]})\n",
    "df_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_csv(r\"C:\\Users\\Danny Farone\\Documents\\GitHub\\Data-Science-Job-Salaries\\data\\processed\\Machine Learning Engineer\\Machine_Learning_Engineer_X_Test.csv\")\n",
    "X_train = pd.read_csv(r\"C:\\Users\\Danny Farone\\Documents\\GitHub\\Data-Science-Job-Salaries\\data\\processed\\Machine Learning Engineer\\Machine_Learning_Engineer_y_Test.csv\")\n",
    "y_test = pd.read_csv(r\"C:\\Users\\Danny Farone\\Documents\\GitHub\\Data-Science-Job-Salaries\\data\\processed\\Machine Learning Engineer\\Machine_Learning_Engineer_y_Train.csv\")\n",
    "y_train = pd.read_csv(r\"C:\\Users\\Danny Farone\\Documents\\GitHub\\Data-Science-Job-Salaries\\data\\processed\\Machine Learning Engineer\\Machine_Learning_Engineerr_X_Train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit initial model on the training data\n",
    "logistic = LogisticRegression(max_iter = 10000, random_state = 42)\n",
    "logistic.fit(X_train, y_train)\n",
    "\n",
    "#accuracy of the model on the testing data\n",
    "log_score_test = logistic.score(X_test, y_test)\n",
    "#predictions for the model on the testing data\n",
    "log_pred_test = logistic.predict(X_test)\n",
    "#Precision of the model on the testing data\n",
    "log_precision_test = precision_score(y_test, log_pred_test)\n",
    "#Recall of the model on the testing data\n",
    "log_recall_test = recall_score(y_test, log_pred_test)\n",
    "\n",
    "#Print a simple confusion matrix of the teting data results\n",
    "print(\"Logistic Regression Model Testing Data Confusion Matrix :\")\n",
    "print(confusion_matrix(y_test, log_pred_test)) \n",
    "#Print the scores\n",
    "print(\"Accuracy: \", log_score_test)\n",
    "print(\"Sensitivity: \", log_recall_test)\n",
    "print(\"Specificity: \", log_precision_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set a variable for the logistic intercept\n",
    "log_intercept = logistic.intercept_ \n",
    "beta_0 = log_intercept\n",
    "\n",
    "# extract log reg coefs\n",
    "coef = logistic.coef_[0]\n",
    "coef = np.array(coef)\n",
    "df_coef = pd.DataFrame(coef)\n",
    "df_coef = df_coef.T # transpose to match column names\n",
    "\n",
    "# column names\n",
    "names = X.columns\n",
    "df_coef.columns = names\n",
    "df_coef = df_coef.T # transpose for better view\n",
    "\n",
    "# sort coefficients in ascending order\n",
    "df_coef = df_coef.rename(columns = {0:'logregCV_coeff'})\n",
    "df_coef = df_coef.sort_values('logregCV_coeff')\n",
    "df_coef = df_coef.reset_index()\n",
    "df_coef = df_coef.rename(columns = {'index':'Variable_Names', 'logreg_coeff':'logregCV_coeff'})\n",
    "df_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display barplot of log reg coefs\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x=\"Variable_Names\",y=\"logregCV_coeff\",data= df_coef)\n",
    "plt.xticks(rotation=80)\n",
    "plt.title('The Barplot of Coefficients of Logistic Regression Model')\n",
    "plt.xlabel('Variable Names')\n",
    "plt.ylabel('Value of Coefficient')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataframe of the the True values, False values, True Predictions, and False Predicitons for a confusion matrix\n",
    "cm = pd.DataFrame(confusion_matrix(y_test, log_pred_test), index=['One_Claim','Mutliple_Claims'], \n",
    "                  columns=['One_Claim','Mutliple_Claims'])\n",
    "cm.index.name = 'True'\n",
    "cm.columns.name = 'Predicted'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot ROC curve\n",
    "y_pred_prob = logistic.predict_proba(X_test)[:,1]\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "\n",
    "plt.plot([0, 1], [0, 1],'k--')\n",
    "plt.plot(fpr, tpr, label='Logistic Regression')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Decision Tree ROC Curve')\n",
    "plt.show();\n",
    "\n",
    "# calculate roc curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, logistic.predict(X_test))\n",
    "\n",
    "# roc auc score\n",
    "log_roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "roc_auc_format = 'ROC AUC Score: {0:.4f}'.format(log_roc_auc)\n",
    "print(roc_auc_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores = pd.DataFrame({'Measures':['Accuracy','Sensitivity','Specificity','AUC'],'Logistic':[log_score,log_recall,log_precision,log_roc_auc]})\n",
    "df_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit new model with best parameters determined by gscv on the training data\n",
    "decTree_gs = GradientBoostingClassifier(random_state =42, learning_rate =0.01, max_depth =3, max_leaf_nodes =2, n_estimators =50)\n",
    "decTree_gs.fit(X_train, y_train)\n",
    "\n",
    "#accuracy of the model on the testing data\n",
    "decTree_score_test = decTree_gs.score(X_test, y_test)\n",
    "#predictions for the model on the testing data\n",
    "decTree_pred_test = decTree_gs.predict(X_test)\n",
    "#Precision of the model on the testing data\n",
    "decTree_precision_test = precision_score(y_test, decTree_pred_test)\n",
    "#Recall of the model on the testing data\n",
    "decTree_recall_test = recall_score(y_test, decTree_pred_test)\n",
    "\n",
    "#Print a simple confusion matrix of the teting data results\n",
    "print(\"Decision Tree confusion matrix:\")\n",
    "print(confusion_matrix(y_test, decTree_pred_test), '\\n')\n",
    "\n",
    "#Print the scores\n",
    "print(\"Accuracy: \", decTree_score_test)\n",
    "print(\"Sensitivity: \", recall_test)\n",
    "print(\"Specificity: \", precision_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
